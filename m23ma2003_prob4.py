# -*- coding: utf-8 -*-
"""M23MA2003_prob4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GxQBV9UWO3LGnF5ZHHw0fj5p7QrzIuD6
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

#Categories that we wnat to classify
categories = [
        'rec.sport.baseball', 'rec.sport.hockey',  # Sports
        'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc' # Politics
    ]
data = fetch_20newsgroups(subset = 'all',categories=categories,remove=('headers','footers','quotes'),shuffle=True,random_state=42)
print(data)

label = []
for t in data.target:
  if data.target_names[t] in ['rec.sport.baseball', 'rec.sport.hockey']:
    label.append(1) #sports
  else:
    label.append(0) #politics

print(f"Number of documents: {len(data.data)}")
print(f"class distribution :{np.bincount(np.array(label))}(0 Politics ,1 Sports)")

X_train,X_test,y_train,y_test = train_test_split(data.data,label,test_size=0.2,random_state=42)

print(len(X_train))

print(X_train[0])

results = {}

"""Naive Bayes"""

model1 = MultinomialNB()
vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)
pipeline = Pipeline([
    ('tfidf',vectorizer),
    ('clf',model1),
])
#Train
pipeline.fit(X_train,y_train)
#Test
y_pred = pipeline.predict(X_test)
acc1 = accuracy_score(y_test,y_pred)
print(f"Accuracy : {acc1:.4f}")
print(f"\n Classification Report\n")
print(classification_report(y_test,y_pred))

#Plot
cm = confusion_matrix(y_test,y_pred)
plt.figure(figsize=(10,10))
sns.heatmap(cm,annot=True,fmt = 'd',cmap = 'Blues',
            xticklabels = ['Politics','Sports'],
            yticklabels = ['Politics','Sports'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
# results['Naive Bayes'] = acc1

"""## Logistic Regression Model"""

model2 = LogisticRegression(max_iter=1000)
vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)
pipeline = Pipeline([
    ('tfidf',vectorizer),
    ('clf',model2),
])
#Train
pipeline.fit(X_train,y_train)
#Test
y_pred = pipeline.predict(X_test)
acc2 = accuracy_score(y_test,y_pred)
print(f"Accuracy : {acc2:.4f}")
print(f"\n Classification Report\n")
print(classification_report(y_test,y_pred))

#Plot
cm = confusion_matrix(y_test,y_pred)
plt.figure(figsize=(10,10))
sns.heatmap(cm,annot=True,fmt = 'd',cmap = 'Blues',
            xticklabels = ['Politics','Sports'],
            yticklabels = ['Politics','Sports'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
# results['Logistic Regression'] = acc2

"""SVC Model"""

model3 = LinearSVC(random_state=42,dual='auto')
vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)
pipeline = Pipeline([
    ('tfidf',vectorizer),
    ('clf',model3),
])
#Train
pipeline.fit(X_train,y_train)
#Test
y_pred = pipeline.predict(X_test)
acc3 = accuracy_score(y_test,y_pred)
print(f"Accuracy : {acc3:.4f}")
print(f"\n Classification Report\n")
print(classification_report(y_test,y_pred))

#Plot
cm = confusion_matrix(y_test,y_pred)
plt.figure(figsize=(10,10))
sns.heatmap(cm,annot=True,fmt = 'd',cmap = 'Blues',
            xticklabels = ['Politics','Sports'],
            yticklabels = ['Politics','Sports'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
# results['SVC'] = acc3

features = {
    'bagofwords':CountVectorizer(stop_words='english',max_features=5000),
    'TFIDF':TfidfVectorizer(stop_words='english',max_features=5000),
    'bigram':TfidfVectorizer(ngram_range=(1,2),stop_words='english',max_features=5000),
}
for f ,v in features.items():
  pipeline = Pipeline([
      ('f',v),
      ('clf',LinearSVC(random_state=42,dual='auto'))
  ])
  pipeline.fit(X_train,y_train)
  y_pred = pipeline.predict(X_test)
  acc = accuracy_score(y_test,y_pred)
  results[f"Linear SVC with {f}"] = acc
  print(f"{f} Accuracy: {acc:.4f}")

features = {
    'bagofwords':CountVectorizer(stop_words='english',max_features=5000),
    'TFIDF':TfidfVectorizer(stop_words='english',max_features=5000),
    'bigram':TfidfVectorizer(ngram_range=(1,2),stop_words='english',max_features=5000),
}
for f ,v in features.items():
  pipeline = Pipeline([
      ('f',v),
      ('clf',LogisticRegression(max_iter=1000))
  ])
  pipeline.fit(X_train,y_train)
  y_pred = pipeline.predict(X_test)
  acc = accuracy_score(y_test,y_pred)
  results[f"Logistic Regression with {f}"] = acc
  print(f"{f} Accuracy: {acc:.4f}")

features = {
    'bagofwords':CountVectorizer(stop_words='english',max_features=5000),
    'TFIDF':TfidfVectorizer(stop_words='english',max_features=5000),
    'bigram':TfidfVectorizer(ngram_range=(1,2),stop_words='english',max_features=5000),
}
for f ,v in features.items():
  pipeline = Pipeline([
      ('f',v),
      ('clf',MultinomialNB())
  ])
  pipeline.fit(X_train,y_train)
  y_pred = pipeline.predict(X_test)
  acc = accuracy_score(y_test,y_pred)
  results[f"Naive Bayes with {f}"] = acc
  print(f"{f} Accuracy: {acc:.4f}")

best = max(results,key=results.get)
print(f"Best Model :{best} with accuracy : {results[best]:.4f}")